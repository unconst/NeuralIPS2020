\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{radford2019language}
\citation{radford2019language}
\citation{devlin2018bert}
\citation{radford2019language}
\citation{wang2018glue}
\citation{radford2019language}
\citation{chollet2019measure}
\citation{chollet2019measure}
\citation{Riabinin2020learningathome}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Methodology}{2}{section.1}\protected@file@percent }
\newlabel{methodology}{{1}{2}{Methodology}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Benchmark}{2}{subsection.1.1}\protected@file@percent }
\citation{hinton2015distilling}
\citation{kaiser2017model}
\citation{alex2014cortical}
\citation{lample2019crosslingual}
\citation{lecun1989optimalbraindamage}
\citation{yu2017nisp}
\citation{yu2017nisp}
\citation{yu2017nisp}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $n=6$ parameterized functions with losses $\mathcal  {L}_i$ and datasets $D_i$.\relax }}{3}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Ideal Ranking}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Inter Ranking}{3}{subsection.1.3}\protected@file@percent }
\newlabel{sec:inter-ranking}{{1.3}{3}{Inter Ranking}{subsection.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Inter-model contribution weights: $w_{i,j}$ the score attributed to $f_j$ from $f_i$\relax }}{3}{figure.caption.3}\protected@file@percent }
\citation{Bathina1989neurotrophin}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Stake}{4}{subsection.1.4}\protected@file@percent }
\newlabel{sec:stake}{{1.4}{4}{Stake}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Competitive weights}{4}{subsection.1.5}\protected@file@percent }
\newlabel{sec:competitive_weights}{{1.5}{4}{Competitive weights}{subsection.1.5}{}}
\citation{shazeer2017outrageously}
\citation{shazeer2017outrageously}
\citation{Riabinin2020learningathome}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inflation mechanism\relax }}{5}{algorithm.1}\protected@file@percent }
\newlabel{eq:7}{{7}{5}{Competitive weights}{equation.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Running the network}{5}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Conditional computation}{5}{subsection.1.7}\protected@file@percent }
\citation{hinton2015distilling}
\citation{Sanh2019DistilBERT}
\citation{balduzzi2020smooth}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Extracting knowledge}{6}{subsection.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Queries propagate to depth=1 before the distilled model is used.\relax }}{6}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Analysis}{6}{section.2}\protected@file@percent }
\newlabel{analysis}{{2}{6}{Analysis}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Payoff Model}{6}{subsection.2.1}\protected@file@percent }
\citation{dtting2017optimal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Empirical Model}{7}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{7}{section.3}\protected@file@percent }
\newlabel{experiments}{{3}{7}{Experiments}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Correlations between the competitive rank and coordinated rank for $\frac  {\alpha }{\tau } \in \{1, 10, 25, 50\}$. For low values of $\frac  {\alpha }{\tau }$ the weights converge to the identity: the state where peers are fully disconnected. \relax }}{8}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{coordvscomp}{{4}{8}{Correlations between the competitive rank and coordinated rank for $\frac {\alpha }{\tau } \in \{1, 10, 25, 50\}$. For low values of $\frac {\alpha }{\tau }$ the weights converge to the identity: the state where peers are fully disconnected. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (i) Left: the ratio between the main diagonal and the remainder of the weights. Right: the adaptive $\tau $ parameter converging onto the target. The weight matrix sparsity is a proxy for the ranking accuracy which we see in Figure~\ref  {coordvscomp}. As sparsity converges onto the target the ranking is also improving.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{adaptive_tau}{{5}{8}{(i) Left: the ratio between the main diagonal and the remainder of the weights. Right: the adaptive $\tau $ parameter converging onto the target. The weight matrix sparsity is a proxy for the ranking accuracy which we see in Figure~\ref {coordvscomp}. As sparsity converges onto the target the ranking is also improving.\relax }{figure.caption.6}{}}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{radford2019language}{1}
\bibcite{devlin2018bert}{2}
\bibcite{wang2018glue}{3}
\bibcite{chollet2019measure}{4}
\bibcite{Riabinin2020learningathome}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Discussion}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{9}{section.4}\protected@file@percent }
\newlabel{conclusion}{{4}{9}{Conclusion}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Broader Impact}{9}{section.5}\protected@file@percent }
\bibcite{hinton2015distilling}{6}
\bibcite{kaiser2017model}{7}
\bibcite{alex2014cortical}{8}
\bibcite{lample2019crosslingual}{9}
\bibcite{lecun1989optimalbraindamage}{10}
\bibcite{yu2017nisp}{11}
\bibcite{Bathina1989neurotrophin}{12}
\bibcite{shazeer2017outrageously}{13}
\bibcite{Sanh2019DistilBERT}{14}
\bibcite{balduzzi2020smooth}{15}
\bibcite{dtting2017optimal}{16}
\citation{lecun1989optimalbraindamage}
\citation{yu2017nisp}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Deriving the idealized ranking.}{10}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Deriving the weight convergence game.}{10}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Theorem}{10}{subsection.6.3}\protected@file@percent }
\citation{dtting2017optimal}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Setup}{11}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Deriving the ex-post zero-regret step.}{11}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Definition}{12}{subsubsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Theorem}{12}{subsubsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.3}Proof}{12}{subsubsection.6.5.3}\protected@file@percent }
