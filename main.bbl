% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language
  models are unsupervised multitask learners,'' \emph{OpenAI Blog}, vol.~1,
  no.~8, p.~9, 2019.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman, ``Glue: A
  multi-task benchmark and analysis platform for natural language
  understanding,'' \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{chollet2019measure}
F.~Chollet, ``On the measure of intelligence,'' \emph{arXiv preprint
  arXiv:/1911.01547}, 2019.

\bibitem{Riabinin2020learningathome}
M.~Riabinin and A.~Gusev, ``Learning@home: Crowdsourced training of large
  neural networks using decentralized mixture-of-experts,'' \emph{arXiv
  preprint arXiv:/2002.04013}, 2020.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean, ``Distilling the knowledge in a neural
  network,'' \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{kaiser2017model}
L.~Kaiser, A.~N. Gomez, N.~Shazeer, A.~Vaswani, N.~Parmar, L.~Jones, and
  J.~Uszkoreit, ``One model to learn them all,'' 2017.

\bibitem{alex2014cortical}
M.~A. Nugent and T.~W. Molter, ``Cortical processing with thermodynamic-ram,''
  2014.

\bibitem{lample2019crosslingual}
G.~Lample and A.~Conneau, ``Cross-lingual language model pretraining,'' 2019.

\bibitem{lecun1989optimalbraindamage}
Y.~LeCun, D.~J. S, and S.~S. A, ``Optimal brain damage,'' \emph{Advances in
  Neural Information Processing Systems 2 (NIPS)}, 1989.

\bibitem{yu2017nisp}
R.~Yu, A.~Li, C.-F. Chen, J.-H. Lai, V.~I. Morariu, X.~Han, M.~Gao, C.-Y. Lin,
  and L.~S. Davis, ``Nisp: Pruning networks using neuron importance score
  propagation,'' 2017.

\bibitem{Bathina1989neurotrophin}
B.~S and D.~UN, ``Brain-derived neurotrophic factor and its clinical
  implications,'' \emph{Arch Med Sci. 2015;11(6):1164–1178.
  doi:10.5114/aoms.2015.56342}, 2015.

\bibitem{Sanh2019DistilBERT}
L.~C. J. W.~T. Sanh, Victor;~Debut, ``Distilbert, a distilled version of bert:
  smaller, faster, cheaper and lighter,'' \emph{arXiv preprint
  arXiv:1910.01108}, 2019.

\bibitem{balduzzi2020smooth}
D.~Balduzzi, W.~M. Czarnecki, T.~W. Anthony, I.~M. Gemp, E.~Hughes, J.~Z.
  Leibo, G.~Piliouras, and T.~Graepel, ``Smooth markets: A basic mechanism for
  organizing gradient-based learners,'' 2020.

\bibitem{dtting2017optimal}
P.~Dütting, Z.~Feng, H.~Narasimhan, D.~C. Parkes, and S.~S. Ravindranath,
  ``Optimal auctions through deep learning,'' 2017.

\end{thebibliography}
