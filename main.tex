\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{siunitx}
\usepackage{authblk}
\usepackage{subcaption}
\usepackage{bbm}
\usepackage{amsmath}

\graphicspath{ {.} }
\include{Commands}
\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=C,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\title{BitTensor: A Peer-to-Peer Intelligence Benchmark}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\


\begin{document}

\maketitle

\begin{abstract}
The dominant tools used to guide machine intelligence are typically benchmarks which rank systems on a set of predefined tasks. However, task performance is a narrowly defined, low-resolution, and non-collaborative measure making it an inefficient guide for machine intelligence. Instead, we propose a benchmark that measures \textit{knowledge production} from within a network of intelligence systems. In this paper we describe how this benchmark is constructed and how it is negotiated by computers that share what they learn peer-to-peer (P2P) across the internet. Since this internet based negotiation introduces game-theoretic considerations, we design methods to improve the benchmark's accuracy when peers remain self-interested and trust is diminished. We then test our design by empirically showing that the negotiation closely matches the desired ranking at the system's competitive equilibrium. 

%The result is a benchmark that is inherently collaborative, efficient, and decentralized.
\end{abstract}

Increasingly, state-of-the-art machine learning systems focus on the production of \textit{machine knowledge}: non-task specific understanding of inputs which can be later tuned to a wide variety of problems \cite{devlin2018bert}. This shift is a consequence of the fact that a focus on tasks during training produces narrow specialists rather than resilient generalists \cite{radford2019language}. However, while becoming the dominant technique for training individual models, the field still opts for task based objectives as the method of reward at the highest level of its evolution \cite{wang2018glue}. We suggest this introduces wide-scale inefficiencies into the advancement of machine intelligence for at least the following reasons:

\begin{enumerate}
	\item Guiding machine intelligence with performance on a  small number of tasks converges the field towards narrow specialists \cite{chollet2019measure}. Unfortunately, simply expanding the set tasks to cover the general problem would require a large number of expensive supervised (dataset, task) pairs \cite{radford2019language}. This intractability suggests task-based feedback mechanisms cannot alone guide machine intelligence towards a high level of generalism. In contrast, task agnostic knowledge can be extracted using unsupervised techniques and measured against a near infinite amount of cheap unlabelled data \cite{devlin2018bert}. The fact that this knowledge can then be used to solve many tasks \cite{radford2019language} suggests that it may also provide a much broader definition of intelligence against which to benchmark. 
	
	\item Canonical task based intelligence benchmarks produce a small number of scores (i.e accuracy on 8 tasks \cite{wang2019superglue}) to measure the performance of the models which they evaluate. While conveniently human interpretable, the scores are low resolution substitutes for the information produced by the models. The language model BERT, for instance, produces a 1024 dimensional continuous distribution over sentences; an ensemble could directly learn from this distribution and rank it in combination with others \cite{shazeer2017outrageously}. By lacking this resolution benchmarks fail to reward niche machine intelligence systems and the knowledge they have uncovered.
	
	\item Models trained on tasks produce knowledge which is fine tuned to the  specific task and therefore not appropriate for sharing with models solving other tasks. In contrast, machine knowledge can -- and increasingly is \cite{Sanh2019DistilBERT} -- shared between research teams to avoid repeated work. A focus on rewarding tasks performance only indirectly rewards this shareable knowledge -- for instance, via attribution in a paper describing the training method -- rather than directly in the benchmark. This tenuous form of attribution means teams are not well incentivized to produce and share this salient element. Instead they are pitted against each other and centralized towards those groups capable of expending the incredible resources it takes to train a benchmark beating model.
\end{enumerate}

What is needed is benchmark that directly measures machine knowledge production \footnote{“The iron rule of nature is: you get what you reward for. If you want ants to come, you put sugar on the floor.” - Charlie Munger}. Our proposal for achieving this is a framework in which models directly share knowledge that they have learned with each other, having their peers rank that knowledge for its ability to improve their own understanding. Crucially, peers may rank their neighbors directly against task-agnostic objectives, for instance unsupervised language modelling, this allows the benchmarks to reward multi-task understanding explicitly and use cheap unlabelled datasets. Furthermore, value against these objectives can be measured directly by combining the distributions from many models. The expanded resolution allows the benchmark to reward peers for niche improvements in understanding. And finally, because knowledge sharing is inherent to the benchmark it allows direct attribution towards models which improve collective understanding. 

Practically, we design the benchmark to run in a continuous and asynchronous fashion, peer-to-peer (P2P) across the Internet. P2p interaction is the most direct and subsequently removes the need for a centralized arbiter. However,  since this introduces trustless computation \footnote{ In decentralized systems, trust is shifted from the individuals to the network itself, "trustless" usually means "minimal trust is required" with respect to the protocol} we dedicate a part of this paper to explaining how the system remains fair when little assurance can be given about the computers that compose it. In summary, the contributions of this paper are thus four-fold:
\begin{enumerate}
	\item We introduce a P2P based intelligence framework.
	\item We introduce a benchmark that rewards models that improve representational knowledge within a network.
	\item We show how models can share knowledge to increase each other's performance, which removes the need for others to re-learn that knowledge. 
	\item We show how a self-interested desire to minimize a loss function can be used to guide the network towards a fair ranking.
\end{enumerate}

The rest of this paper is organized as follows: Section~\ref{methodology} presents the intelligence benchmark and ranking mechanism that models use to evaluate each other. Section~\ref{sec:market} describes the methods we use to detect and respond to the scenario where participants are not honestly reporting the significance of their peers. Section~\ref{analysis} describes the empirical model we use to test the network's incentive structure and Section ~\ref{experiments} highlights the experiments performed to verify the function this framework. Finally, Section~\ref{conclusion} presents a summary and future works on this system.

\section{Methodology}
\label{methodology}
\subsection{Benchmark}

The p2p network is composed of $n$ unique parameterized functions $F = {f_0, ...,  f_j, ...f_n}$ where each function is produces an output tensor $f_i(F(x))$, a ``representation" from an input tensor $F(x) = [f_0{(x)} ... f_n{(x)}]$ gathered by querying its neighbors. Each function trains asynchronously over a dataset $D_i=[X,Y]$ such that, given an error function $\mathcal{Q}_i$, its expectation over that data $E_{Di}$ defines a loss $\mathcal{L}_i = E_{Di}[\mathcal{Q}_i( \ y, \ f_i(F(x)) \ )]$. We assume these losses are measured on the same scale and thus our benchmark $\mathcal{B}$ can be defined by their sum:

\begin{equation}
\mathcal{B} = \ \sum_{i}^{n} \mathcal{L}_i 
\end{equation}

\begin{figure}[H]
	\centering
	\hspace*{-2cm}
	\input{Neurons.tex}
	%    \label{fig:progdense_diagram}
	\caption{$n=6$ parameterized functions with losses $\mathcal{L}_i$ and datasets $D_i$.}
\end{figure}{}

Each parameterized function is represented here abstractly as function \cite{hinton2015distilling} and need only accept the same input type $x$ and produce the same output dimension to fit within the network. For instance, input may be a unicode encoded string $x = \text{"hello"}$ and output its semantic representation as a $1024$ dimensional word embedding. This widened scope ensures participants can be multi-task \cite{kaiser2017model}, use completely distinct computing substrates \cite{alex2014cortical} or train on unique datasets. \cite{lample2019crosslingual}. 

\subsection{Ideal Ranking}

Our goal in this work is to produce a ranking $R = [R_i]$ over these functions where the score $R_i \in R$ represents participant $i$'s information-theoretic significance to the benchmark $\mathcal{B}$. Following Le Cun and others \cite{lecun1989optimalbraindamage,yu2017nisp}, it is reasonable to define this significance by equating it with the cost of removing each component from the network. We derive this analytically as follows:

\begin{equation}
\label{eq:r}
R_i \approx \ \ \frac{1}{n} \sum_{j}^{n} \sum_{x \in D_j} \Delta F^T(x)_i * H(\mathcal{Q}_j(x)) * \Delta F(x)_i 
\end{equation}

\[ \Delta F (x)_i = [0, ... 0, -f_i(x), 0, ... 0] \]

 This can be derived via Taylor series where $\Delta F (x)_i$ is the perturbation of the $j^{th}$ node's inputs when the $i^{th}$ node is removed from the network at the point $(x)$ (Appendix 6.1). The remaining term $H(\mathcal{Q}_j)$ is the hessian of the $j^{th}$ node's error function \cite{yu2017nisp}. When the error function $\mathcal{Q}_j$ is the twice-differentiable cross-entropy then $H(\mathcal{Q}_j)$ is it's Fishers information matrix. In this form, $R_i \in R$ is suitably measured as relative entropy reflecting each participants \textit{informational significance} to the network as a whole.

\subsection{Inter Ranking}
\label{sec:inter-ranking}
It is not possible to compute the ranking score above without access to the parameters of each function in the network. Instead we use an $n \times n$ matrix $W = [w_{i,j}]$ where each $w_{i,j}$ is the score attributed to the $j^{th}$ node from the $i^{th}$. 

\begin{equation}
\label{eq:w}
w_{ij} = \ \ \sum_{x \in D_i} \Delta F^T(x)_j * H(\mathcal{Q}_i(x)) * \Delta F(x)_j
\end{equation}

\begin{figure}[H]
	\centering
	\hspace*{-2cm}
	\input{Weights.tex}
	%    \label{fig:progdense_diagram}
	\caption{Inter-node contribution weights: $w_{i,j}$ the score attributed to $f_j$ from $f_i$}
\end{figure}{}

The equivalent ranking $R$ in Equation ~(\ref{eq:r}) can then be computed by taking the column sum of the weight matrix like so:

\begin{equation}
\label{eq:equiv_ranking}
R = \frac{1}{n} W^T * \mathbbm{1}
\end{equation}

The weights can be computed on the fly either approximately -- e.g. by using a heuristic to propagate a contribution score from the error function through to the input \cite{yu2017nisp} -- or by using the full hessian of the error as described in Equation ~(\ref{eq:r}). In practice we store these weights and compute the ranking on a distributed ledger as i. Participants can then make updates to this ledger by submitting changes of bounded size: $W^{t+1}= W^t + \lambda \Delta W$. The ledger enforces the learning rate $||W_i||_2 < \epsilon$, and that rows remain suitably normalized $||w||_1 = 1$ at each block step $t$.  

\section{Market}
\label{sec:market}

The immediate problem with the ranking described above is that without system-wide access to the model parameters, the computation of $w_{ij}$ in Equation (\ref{eq:w}) is non-auditable: testing the truthfullness of the weights would require access to the parameters of each model, information we do not have on the distributed ledger. It is reasonable to assume participants will select weights which artificially increase their own rank rather than others in the network -- therefore undermining the validity of the benchmark. The primary suggestion in this paper however, is that we can attain an accurate ranking even when self-interested behaviour is assumed and the weights are non-auditable.  

\subsection{Stake}
\label{sec:stake}
Our proposal is to frame the benchmark as a market by introducing a finite resource $ S=[s_i]$, a component's 'stake' in the system, which gives them more weight to rank others. Simultaneously the mechanism translates the ranking vector $R$ into additional stake as incentive, providing inflation to those with the highest scores. The 'tokenization' of power in the benchmark and its inflation mechanism serves the following purposes: 

\begin{enumerate}
    \item By controlling the distribution of power within the network the mechanism can guide the system with reward signals towards the fair ranking. \footnote{The propagation of a value holding resource mimics the flow of the brain-derived neurotrophic factor (BDNF) in the brain \cite{Bathina1989neurotrophin}.} 
	\item Payment in the form of mechanism power 'stake' to those with large rank ensures that those with weight must have worked to attain it, or indirectly subsidized those who have done so already. 
    \item While computers can spuriously create new nodes in the network they cannot spuriously create mechanism power since this remains finite in the system.
\end{enumerate}

Our practical implementation of the stake mechanism is described in Algorithm 1.  

\begin{algorithm}
	\caption{Inflation mechanism}
	\begin{algorithmic} 
		\REQUIRE $S = \ [n \times 1] \ \textrm{Stake Vector}$
		\REQUIRE $W = \ [n \times n] \ \textrm{Weight Matrix}$
		\REQUIRE $\tau > 0 \ \textrm{inflation rate}$
		\WHILE{$\text{TRUE}$}
		\STATE $W = W + \lambda \Delta W$
		\STATE $R = \frac{1}{n} W^T \circ S * \mathbbm{1}$
		\STATE $S = S + \tau * \frac{R}{||R||_2}  $
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

While stake provides some protection against malicious actors, it does not ensure weights are set accurately. Indeed, it may even provide further incentive for those computers to increase their own weight (or their friends) at the expense of others. 

\subsection{Competition}
\label{sec:competitive_weights}

Our solution begins by introducing competition for connectivity within the network. Nodes that underweight are punished by having inputs from the network masked to zero~(\ref{eq:7}), which would in turn punish the loss function. To frame this market we borrow the continuous differential activation function $\sigma$ with range $(0,1)$. Under a choice of weights $W_i$ the inputs to component $i$ are:

\begin{equation}
\label{eq:7}
F_W(x) =  [f_0(x) * \sigma(s_i * w_{i,0} - \mu_0),  ... , f_n(x) * \sigma(s_i * w_{i,n} - \mu_n)]
\end{equation}

\begin{equation}
\sigma =  \frac{1}{ 1 + e^{-\frac{x}{T}} }
\end{equation}

Here, the shift term $\mu_j$ is the average of the weights in each column $\mu_j = (\frac{1}{n}) \sum_{i}^{n}{s_i * w_{i,j}}$, and the activation function is the temperature scaled sigmoid. Because the allocation mechanism is standard across the network it is possible for each participants to compute both $\frac{\partial \mathcal{L}_i}{\partial W_i}$ and $\frac{\partial R_i}{\partial W_i}$. Computers may augment their usual training framework, for instance, Tensorflow, with the allocation mechanism shown here. 

\subsection{Running the network}

The steps to run a network participant are:
\begin{enumerate}
	
	\item Participant defines its dataset $D_i$, loss $\mathcal{L}_i$ and parameterized function $f_i$
	\item  At each training iteration the participant broadcasts batches of examples from $D_i$ to its peers $[\textit{batch\_size}, x]$
	\item Responses $F(x)$ from the network produce a loss-gradient $\frac{\partial \mathcal{L}}{\partial F}$ which back-propagates through $f_i$ and out to the network.
	\item  During 2 and 3 the participant competitively selects the weights for their row $w_{ij} \in W$.
	\item  Participants submit changes to the weights $\Delta W_i$ which, in-turn changes the ranking and induces inflation $\tau * R$.
	\item  After steps 1-5 have been performed multiple times, participants disconnect and verify the model in a normal manner.
\end{enumerate}

Peers only communicate with computers that hold stake as a consequence of section~\ref{sec:competitive_weights}. Those that fail to produce value will be pruned naturally as participants learn to differentiate signal from noise.

% \subsection{Conditional computation}

% As the network grows, outward bandwidth will become the major bottleneck. Components learn to trim outward bandwidth by employing a Sparsely-Gated Mixture-of-Experts (SGMoE) \cite{shazeer2017outrageously} layer at the input. The gating layer determines a sparse combination of children to query for each example and then re-joins them using the the gating weights $g_j(x)$. The combined gated inputs are fed as input to the local function: 

% \begin{equation}
% f_i = f_i(G(x)) \ \ \ \  \textrm{ }
% \end{equation}

% \begin{equation}
% \label{eq:gating_ensemble}
% G(x) = [ ..., g_j(x) * f_j(x), ...]
% \end{equation}


% The layer cuts outward bandwidth, querying only a small subset of peers for each example. The gating function is trainable w.r.t to the loss and its weights act as a proxy for importance $w_{ij} \in W$. This method has been shown to drastically increase the potential for outward bandwidth in datacenter training,\cite{shazeer2017outrageously} and has been investigated in a peer-to-peer setting as well \cite{Riabinin2020learningathome}


% \subsection{Extracting knowledge}

% Inter-node dependence in the network is broken using distillation\cite{hinton2015distilling}, a compression and knowledge technique in which a smaller model -- the student - mimics the behaviour of an ensemble. We employ this technique over the gating ensemble described in Equation (\ref{eq:gating_ensemble}) where the student model learns to minimize the cross-entropy (shown below as KL) between the logits produced by the gating network and its predicted distribution. \cite{Sanh2019DistilBERT}

% \begin{equation}
% \textrm{distillation loss} = \text{KL}_D(\text{dist}(x), G(x)) 
% \end{equation}

% We use the distilled model as proxy to cut recursive calling between each components rather than query farther into the network. If models go offline, their peers can use their distilled versions in-place. Private data can be validated over the distilled models instead of querying the network. Eventually, components can fully disconnect from the network using the distilled inputs to validate and inference the models offline.

% \begin{figure}[H]
% 	\centering
% 	\hspace*{0cm}
% 	\input{distillation.tex}
% 	%    \label{fig:progdense_diagram}
% 	\caption{Queries propagate to depth=1 before the distilled model is used.}
% \end{figure}{}

\section{Analysis}
\label{analysis}

We consider the scenario where participants are not honestly reporting the significance of their peers. The network is progressing at discrete timesteps $t$ and it is reasonable to assume that each participant is attempting to maximize their subjective payoff over these steps.

\subsection{Payoff Model}

The staking system in Section~\ref{sec:stake} gives participants incentive to maximize their self-weight $w_{ii}$ while the competitive connectivity described in Section~\ref{sec:competitive_weights} makes it costly for participants to decrease the remaining weights $w_{ij}$ in their row. Since the row must sum to 1, we have a trade-off in two terms:

\begin{enumerate}
	\item A utility term attached to the loss $U(\mathcal{L}(W))$.
	\item The token emission via inflation $\tau * R_i(W)$. 
\end{enumerate}

Both of these are functions of the weights and, without loss of generality, measured in similar units:

\begin{equation}
P_i (W) = U_i(\mathcal{L}_i(W)) + \tau * R(W)_i
\end{equation}

It is reasonable to assume payoff maximizing participants will use $P_i$ as their objective during training. This can be computed using standard tools since both terms $U(\mathcal{L}_i(W))$ and $R(W)_i$ are fully continuous and differentiable. The system can be characterized as a competitive gradient descent game where participants are making steps $\Delta W_i = \frac{\partial P_i}{\partial W_i}$. In appendix 6.2 we prove that this strategy is regret-free and achieves the best expected payoff in hindsight. This assumption is also generally employed in smooth markets \cite{balduzzi2020smooth}. The iterative descent thus follows:


\begin{equation}
W^{t+1} = W^{t} + \lambda \Delta W 
\end{equation}

\begin{equation}
\label{eq:grad_step2}
\Delta W = [\frac{\partial P_0}{\partial W_0} ; ... ;\frac{\partial P_n}{\partial W_n}]
\end{equation}

\subsection{Empirical Model}

Without access to the running network we evaluate our system using an empirical model. To derive the gradient steps in this model $\frac{\partial P_i}{\partial W_i}$ we make the following assumptions: 

\begin{enumerate}
	\item The utility functions are continuous-differentiable and their first order derivatives are bounded bellow $\frac{\partial U}{\partial \mathcal{L}} > \alpha$, $\alpha > 0$.     
	\item  The network is converged to a local minimum in the inputs $\frac{\partial\mathcal{L}}{\partial F} = 0$.
\end{enumerate}  

The first assumption guarantees that nodes derive some non-zero value from minimizing their loss. We assume that this value is positive but but make no assumptions about its scale. Instead we intend to explore the relationship between alpha and the resultant ranking. The second assumption is realistic after extended training where nodes have reached a steady point. In Appendix 6.2 we derive the following gradient step:

\begin{equation}
\label{eq:iterative_descent1}
\frac{\partial P}{\partial W} = \frac{\alpha}{\tau} * \frac{\partial L}{\partial W} + \frac{\partial R}{\partial W}
\end{equation}


\begin{equation}
\label{eq:iterative_descent2}
\frac{\partial L}{\partial W} = \frac{\partial}{\partial W} [(F_W - F_{W_0})^T * H( \mathcal{L}(F)) * (F_W - F_{W_0})] 
\end{equation}

Here $F_W$ is the masked inputs from Section~\ref{sec:competitive_weights}. $(F_W - F_{W_0})$ is the difference in the mask between the choice of weights $W$ and the weights at the minimum $W_0$ and $H( \mathcal{L}(F))$ is the $[n \times n]$ hessian of the loss over inputs $F$. This formulation is both intuitive and useful: the gradient term $\frac{\partial L}{\partial W}$ measures the change in loss for any choice of weights, while the second term is the change in rank with respect to a change in weights $\frac{\partial R}{\partial W}$. The balance of these two terms is guided by $\frac{\alpha}{\tau}$ which represents the ratio between each nodes subjective desire to minimize their loss and maximize their ranking.

The remainder of the network is deterministic, and can be described by a choice of $\theta = [\alpha, \tau, \lambda, n, \sigma, T, S, W, H]$. For instance, the secondary term, $\frac{\partial R}{\partial W}$ can be computed-directly from Section~\ref{sec:stake} and only depends on the stake vector $S$ and weights $W$.

\section{Experiments}
\label{experiments}
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=.3]{images/c_vs_c_nice.png}
	\caption{Correlations between the competitive rank and coordinated rank for $\frac{\alpha}{\tau} \in \{1, 10, 25, 50\}$. For low values of $\frac{\alpha}{\tau}$ the weights converge to the identity: the state where peers are fully disconnected. }
	\label{coordvscomp}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=.27]{images/adaptive_tau.png}
	\caption{(i) Left: the ratio between the main diagonal and the remainder of the weights. Right: the adaptive $\tau$ parameter converging onto the target. The weight matrix sparsity is a proxy for the ranking accuracy which we see in Figure~\ref{coordvscomp}. As sparsity converges onto the target the ranking is also improving.}
	\label{adaptive_tau}
\end{figure}


To generate sample statistics from the network, we first select $[\alpha, \tau, \lambda, S, \sigma, T, n] \in \theta$, then we generate random positive semi-definite hessians $H$, and random uniform initial weights $W_0$. For each parameterization we discover the competitive ranking by converging the system to a Nash-equilibrium: an equilibrium where no individual can vary from their set of weights and stand to gain \cite{dtting2017optimal}. To find this equilibrium, we use the competitive descent strategy described in (\ref{eq:iterative_descent2}) and compute the gradient terms from (\ref{eq:grad_step2}). In each trial we use a learning rate $\lambda = 0.005$ and stop when the gradient terms are bounded by $\epsilon$ or the steps exceed $1000 \times n$. The competitive ranking $R^*$ at this point follows from (\ref{eq:equiv_ranking}) and can be compared to the idealized score in $R$ from (\ref{eq:r}).

We are interested in the correlation between $R^*$ and $R$ as we vary the ratio $\alpha/\tau$, this ratio is explicit in (\ref{eq:iterative_descent1}) where the fraction relates the two gradient terms. Intuitively the ratio is between the value of minimizing the loss and maximizing revenue from inflation. Since this effects the ranking, we show this trade-off for various choices in Figure~\ref{coordvscomp}. 

Finally, in Figure-\ref{adaptive_tau} we implement an adaptive-$\tau$ strategy where the network varies the inflation rate. Initial inflation is zero and then increases until the weights begins to converge towards the main diagonal $w_{ii}=1$. We measure the sparsity by: 

\begin{equation}
\text{sparsity} = \frac{\sum W_{dg}}{\sum W}
\end{equation}

as the ratio between the main diagonal and the remaining weights. As sparsity increases we push the market equilibrium by decreasing $\tau$. Figure~\ref{adaptive_tau} shows this adaptive convergence for $\alpha=1$ with a sparsity target of 1. 

\subsection{Discussion}

Figure~\ref{coordvscomp} shows the relationship between the idealized rank and the competitive rank as a function of $\frac{\alpha}{\tau}$. Figure-\ref{coordvscomp}-a, $\frac{\alpha}{\tau} = 1$ shows the case where this ratio negatively effects the ranking accuracy. Here, all components have set $w_{ii} = 1$ and the resulting scores for all participants has converged to $1/n$. When this occurs, the system could decrease the inflation rate $\tau$ and push the network towards the high information markets seen in Figure-\ref{coordvscomp}-b, c, and d. Figure-\ref{adaptive_tau} shows a basic implementation of this where $\tau$ adapts to the ratio between the row-sums and the main-diagonal. By lowering inflation, it subsequently 'costs less' to connect with peers. Profit maximizing nodes automatically adjust to the change and the system converges back towards an accurate ranking. Those with high ranks will oppose inflation decreased, while those with low ranks will welcome it. The equilibrium found in this meta game will most certainly depend on the number of participants, key to both the ranking accuracy and the market at its core. As well, practically, it should be noted that throttling $\tau$ may be an ineffective tool if we are seeking to achieve a fixed market structure. Other methods, can be implemented, for instance by enforcing magnitude restrictions, with similar effect. We leave these and other considerations for a follow up paper.

\section{Conclusion}
\label{conclusion}
We have proposed a machine intelligence benchmark that can run in a P2P setting outside of a trusted environment. Crucially, the benchmark measures performance as multi-task knowledge production using other intelligence systems, this can be done in a collaborative and high resolution manner. To get there we started with a machine learning benchmark defined by a set of functions with their losses over given datasets. We showed how this framework allowed us to produce an intelligence benchmark based on cost to prune them from the system. Nodes could negotiate this using a set of peer-to-peer weights rather than centralized control. However, the system was incomplete without a mechanism that prevented participants from ranking dishonestly. To resolve this, we proposed an incentive scheme and a differential allocation system over the network weights. The system allowed the participants to train for connectivity in the graph -- ranking competitively in the process. Following this, we described how to increase the outward bandwidth in the system using a trainable gating network and how to cut dependence between nodes using distillation. Finally we designed an empirical model to show how the system converged towards this equilibrium. Using an adaptive inflation mechanism that equilibrium could be guided towards a ranking which correlated with that found in a idealized setting. The result is a fair benchmark which rewards its participants for producing knowledge and making it available to new learners in the system.

%\begin{itemize}
%	\item $n$: The number of nodes e.g. n=100. 
%	\item $\alpha$: The first order utility derivative $\frac{\partial U}{\partial \mathcal{L}} = \alpha$.
%	\item $\tau$: The block-inflation rate: $S^{t+1} = S^t + \tau * R$.
%	\item $\lambda$: The weight matrix learning rate: $W^{t+1} = W^t + \lambda \Delta W$.
%	\item $\sigma$, $T$: The activation function and temperature: $\sigma =  \frac{1}{ 1 + e^{-\frac{x}{T}} }$
%	\item $S$: The stake vector $[1 \times n]$
%	\item $W_0$: A initial weight matrix $ [n \times n]$
%	\item $H( \mathcal{L}(F))_i$: For each $f_i$, the hessian of the loss $L_i$ over inputs $F(x)$ $ n \times [n \times n]$
%\end{itemize}
\newpage

\section{Broader Impact}

From its early conception as a field \cite{turing1950Machinery} up until recently \cite{chollet2019measure} much attention has been given to the measure of intelligence as its definition and a goal for the field, considerably less attention has been given to its incentivization: the manner in which intelligence production is rewarded, what survives through time, what holds our attention long enough to be iterated on, which of its creators receive academic positions, jobs, and monetary rewards. 

These two facets are related -- often we simply reward that which performs the best -- but they are distinctly not the same thing. We know from novelty search \cite{lehman2011Novelty} and exploration in reinforcement learning, for instance, that on occasion it is useful to reward work completely unaligned with the objective. Indeed, even with good measures, bad incentives will ensure the field never achieves them.   

This paper critiques the incentivization structure at the heart of machine intelligence which drives engineers to train models which perform SOTA on task-based benchmarks. We argue that a focus on task based benchmarking as the key incentive mechanism is unsuitable for the efficient realization of strong general artificial intelligence. Our argument rests on three points: (1) That benchmarks are ineffective proxies for general intelligence, (2) that they are low resolution measurement techniques which prune useful work (3) and that they produces a competition which is inherently non-collaborative. 

We attempt to solve these issues by designing a new method of incentivizing intelligence production directly in an online p2p network. While the impact of this design remains untested -- i.e. we cannot know how the system will behave in the wild without having it first exist -- we conjecture the following points about the broader impact of the system should it ever see the light of day. 

First, the benhmark in this paper propses changing the nature of intelligence benchmarks so that that they can directly measure \textit{machine knowledge} without explicitly looking to tasks. This reframing could drastically increase the number of tasks and datasets concurrently being used to guide machine intelligence at the highest level of its evolution. Measurement over feature distributions would change the resolution of benchmarks increasing their ability to differentialy reward multiple models. And finally, incentivize the production of a task agnostic product (machine knowledge) which is inherently shareable.  

Second, we design our benchmark to run across many collaborating peers over the internet. Resources from many teams could be joined and this may drastically the effect the scale of resulting intelligence system. Smaller teams would have the ability to contribute such a system without the needed compute. We know from other distributed peer to peer systems like BitTorrent, that there is incredible amounts of computing resource latent and useable on the internet which could be tapped by such a system.

Third, a peer to peer intelligence system would make the resultant product in the system accessible and open. This may help reduce waste in machine intelligence as those systems already trained could be used by others. At the same time, an open-systems would make these resources reachable by many more people than currently have access. Will open access to AI give power to bad actors? Yes, but centralizing power of intelligence in the hand of the few  may be equally if not more dangerous.

Finally, we have intergrated modern digital ledger technology into the design of the incentive structure. Finite resources in this capacity have been shown to have value carrying qualities close in nature to moneies. If so, the proposed benchmark pays those that improve it. This could provide a suitable avenue for researchers to directly survive off their intelligence production without needing corporate middlemen. This may level the playing the field and bring ai-home to the research organizations from which it first arose. A market for intelligence could stimulate its production in the same way economies in other arenas have. For instance, the economy for hashing power in the case of Bitcoin has increased the scale of that system far beyond the computing power of large corporations or nation states. However, in the same breath that markets are mentioned, we should be careful and admit that markets can be simultaneously treacherous and ineffective in some contexts. These are some of the major concerns that our paper investigates, in fact this is the crux of our work. 




k
\begin{enumerate}
    \item Thwork e benchmark is more aligned with general intelligence, as it is more efficient and more inherently collaborative. 
    \item The introduction of the peer-to-peer based intelligence benchmark creates a new "intelligence economy" in which the best performing models are rewarded to make intelligence.
    \item This also has the potential to allow companies to "purchase intelligence" from researchers, creating a supply-and-demand based economy, one where individuals can be directly rewarded for their work.
    \item The incentive directly rewards the creation of intelligence.  
   
    \item The propagation of knowledge across the network is sped up due to the rewarding mechanism. 
    \item Academically speaking, the protocol also has the potential to resolve the reproducibility problem as researchers will be able to simply place their work on the network and report their contributions in accordance with the benchmark performance. 
\end{enumerate}
 
\newpage


\small
\bibliographystyle{IEEEtran}
\bibliography{references}        % References

\end{document}